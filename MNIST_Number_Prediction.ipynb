{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aae5a39-46ad-47ec-a728-7941df35f498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6e6a61f-3f34-442f-b380-90051f84f9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.keras.utils as tku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25b00564-5695-4bad-8bdc-513464d120bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.19.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b879497b-e82f-4b54-80b3-b809fef77876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37340 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "training_set = train_datagen.flow_from_directory('C:\\\\Users\\\\nikhi\\\\OneDrive\\\\Desktop\\\\PrepInsta DA Course\\\\Projects\\\\datasets\\\\MNIST\\\\training_set',\n",
    "                                                 target_size = (28, 28),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3cc1074-9039-454d-b4a5-093c5184f70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4660 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "test_set = train_datagen.flow_from_directory('C:\\\\Users\\\\nikhi\\\\OneDrive\\\\Desktop\\\\PrepInsta DA Course\\\\Projects\\\\datasets\\\\MNIST\\\\test_set',\n",
    "                                                 target_size = (28, 28),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7dc5cb6-6944-4f0b-9658-1ac110b97cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fffaf7a-ce39-4b5c-a897-ccb250a5a6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikhi\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[28, 28, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfca2a90-aa4b-4e59-aafd-29cc3ef9df45",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1224cf58-4a9a-47ae-8ce5-80260aed8312",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b6af425-c76e-4184-b0a7-e76c55896fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ddb4bca-8b14-4ce2-bdd4-229292577797",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=64, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ab241a2-1ec0-437f-a85b-73a58830d76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2513b4c3-b985-4f6e-bca4-b45a061f4194",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e00a803d-5db2-404d-a39c-1e3a2a7568be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikhi\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1167/1167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m377s\u001b[0m 322ms/step - accuracy: 0.7799 - loss: 0.6896 - val_accuracy: 0.9412 - val_loss: 0.1854\n",
      "Epoch 2/20\n",
      "\u001b[1m1167/1167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 40ms/step - accuracy: 0.9487 - loss: 0.1610 - val_accuracy: 0.9715 - val_loss: 0.0963\n",
      "Epoch 3/20\n",
      "\u001b[1m1167/1167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 44ms/step - accuracy: 0.9627 - loss: 0.1168 - val_accuracy: 0.9700 - val_loss: 0.1007\n",
      "Epoch 4/20\n",
      "\u001b[1m1167/1167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 39ms/step - accuracy: 0.9707 - loss: 0.0938 - val_accuracy: 0.9736 - val_loss: 0.0810\n",
      "Epoch 5/20\n",
      "\u001b[1m1167/1167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9747 - loss: 0.0801 - val_accuracy: 0.9751 - val_loss: 0.0726\n",
      "Epoch 6/20\n",
      "\u001b[1m1167/1167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 37ms/step - accuracy: 0.9791 - loss: 0.0660 - val_accuracy: 0.9749 - val_loss: 0.0774\n",
      "Epoch 7/20\n",
      "\u001b[1m1167/1167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 41ms/step - accuracy: 0.9821 - loss: 0.0568 - val_accuracy: 0.9790 - val_loss: 0.0659\n",
      "Epoch 8/20\n",
      "\u001b[1m1167/1167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 36ms/step - accuracy: 0.9831 - loss: 0.0523 - val_accuracy: 0.9803 - val_loss: 0.0627\n",
      "Epoch 9/20\n",
      "\u001b[1m1167/1167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 42ms/step - accuracy: 0.9837 - loss: 0.0495 - val_accuracy: 0.9811 - val_loss: 0.0570\n",
      "Epoch 10/20\n",
      "\u001b[1m1167/1167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 94ms/step - accuracy: 0.9847 - loss: 0.0467 - val_accuracy: 0.9820 - val_loss: 0.0608\n",
      "Epoch 11/20\n",
      "\u001b[1m1167/1167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 95ms/step - accuracy: 0.9843 - loss: 0.0468 - val_accuracy: 0.9830 - val_loss: 0.0547\n",
      "Epoch 12/20\n",
      "\u001b[1m1167/1167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 99ms/step - accuracy: 0.9863 - loss: 0.0399 - val_accuracy: 0.9818 - val_loss: 0.0560\n",
      "Epoch 13/20\n",
      "\u001b[1m1167/1167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 103ms/step - accuracy: 0.9877 - loss: 0.0396 - val_accuracy: 0.9811 - val_loss: 0.0594\n",
      "Epoch 14/20\n",
      "\u001b[1m1167/1167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 104ms/step - accuracy: 0.9880 - loss: 0.0384 - val_accuracy: 0.9843 - val_loss: 0.0474\n",
      "Epoch 15/20\n",
      "\u001b[1m1167/1167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 100ms/step - accuracy: 0.9880 - loss: 0.0370 - val_accuracy: 0.9863 - val_loss: 0.0479\n",
      "Epoch 16/20\n",
      "\u001b[1m1167/1167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 101ms/step - accuracy: 0.9880 - loss: 0.0354 - val_accuracy: 0.9820 - val_loss: 0.0607\n",
      "Epoch 17/20\n",
      "\u001b[1m1167/1167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 96ms/step - accuracy: 0.9886 - loss: 0.0357 - val_accuracy: 0.9852 - val_loss: 0.0497\n",
      "Epoch 18/20\n",
      "\u001b[1m1167/1167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 106ms/step - accuracy: 0.9907 - loss: 0.0300 - val_accuracy: 0.9815 - val_loss: 0.0582\n",
      "Epoch 19/20\n",
      "\u001b[1m1167/1167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 113ms/step - accuracy: 0.9903 - loss: 0.0305 - val_accuracy: 0.9835 - val_loss: 0.0560\n",
      "Epoch 20/20\n",
      "\u001b[1m1167/1167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 109ms/step - accuracy: 0.9899 - loss: 0.0297 - val_accuracy: 0.9809 - val_loss: 0.0595\n"
     ]
    }
   ],
   "source": [
    "trained_model = cnn.fit(x = training_set, validation_data = test_set, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a869a7a-222a-437e-8f00-27f304e8347b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 369ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('C:\\\\Users\\\\nikhi\\\\OneDrive\\\\Desktop\\\\PrepInsta DA Course\\\\Projects\\\\datasets\\\\MNIST\\\\single_prediction\\\\img_96.jpg', target_size = (28, 28))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "results = (cnn.predict(test_image))\n",
    "training_set.class_indices\n",
    "if results[0][0] == 1:\n",
    "  prediction = 'Zero'\n",
    "elif results[0][1] == 1:\n",
    "  prediction = 'One'\n",
    "elif results[0][2] == 1:\n",
    "  prediction = 'Two'\n",
    "elif results[0][3] == 1:\n",
    "  prediction = 'Three'\n",
    "elif results[0][4] == 1:\n",
    "  prediction = 'Four'\n",
    "elif results[0][5] == 1:\n",
    "  prediction = 'Five'\n",
    "elif results[0][6] == 1:\n",
    "  prediction = 'Six'\n",
    "elif results[0][7] == 1:\n",
    "  prediction = 'Seven'\n",
    "elif results[0][8] == 1:\n",
    "  prediction = 'Eight'\n",
    "else:\n",
    "  prediction = 'Nine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5f6ad36-e34b-44e7-a65b-47377a456479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fd4f9f83-0093-4cc0-9da9-7bc5cf019e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Four\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e66f8e-8acf-4462-8385-ee8b33afd464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
